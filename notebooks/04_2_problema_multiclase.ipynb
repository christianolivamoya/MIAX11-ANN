{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2CZBOKsOZnf"
   },
   "source": [
    "# <font color=\"#CA3532\">Introducción a TensorFlow (parte II)</font> \n",
    "\n",
    "Clasificación multiclase. Ejemplo detallado con el problema Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9GMdFSVORkA"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u26frSE2QcdL"
   },
   "source": [
    "## <font color=\"#CA3532\">Carga de los datos del problema Iris</font>\n",
    "\n",
    "Vamos a empezar con un problema muy sencillo, uno de los problemas de clasificación más famosos que existen. Fue propuesto por R.A. Fisher en 1936, y consiste en clasificar plantas de la especie *Iris* en tres subespecies: *iris-virginica*, *iris-setosa* e *iris-versicolor*. Los atributos que describen cada planta son las dimensiones (longitud y anchura) del pétalo y el sépalo. El conjunto de datos contiene un total de 150 plantas, 50 de cada una de las clases. \n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1aN5FeKpb1SBUhuglGNIe97-HFB0Naiaz)\n",
    "\n",
    "En la siguiente celda cargamos los datos del problema, que están incluidos en el paquete *sklearn.datasets*. En este enlace tienes una descripción de los datos:\n",
    "\n",
    "https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-plants-dataset\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYyw0nTpA461"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAcdpimjSFUs"
   },
   "source": [
    "La variable ``iris`` es un diccionario con los siguientes elementos:\n",
    "\n",
    "- ``data``: array de numpy con los datos del problema (no incluye la clase). Cada fila es un ejemplo (150), cada columna es un atributo (4).\n",
    "- ``target``: array de numpy con las clases de los 150 ejemplos, cada clase es un número entre 0 y 2.\n",
    "- ``target_names``: array de numpy con los nombres de las 3 clases.\n",
    "- ``DESCR``: string con una descripción del problema.\n",
    "- ``feature_names``: lista con los nombres de los 4 atributos.\n",
    "- ``filename``: nombre del fichero que contiene los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Z7MREzcBWiq"
   },
   "source": [
    "### <font color=\"#CA3532\">Visualización de los datos</font>\n",
    "\n",
    "Creamos un DataFrame con los datos para que sea más fácil visualizarlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuP8bkF-Up1M"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target_names[iris.target]\n",
    "df['target_num'] = iris.target\n",
    "df[::10].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDR9sQVFV9oB"
   },
   "source": [
    "Y mostramos la distribución de las clases en el plano definido por cada pareja de atributos. Como vemos el problema no es muy complicado. Una de las clases (setosa) está completamente separada de las otras dos. Las dos últimas solapan ligeramente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRQ4-y0zBQto"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,15))\n",
    "n_classes = 3\n",
    "plot_colors = \"bry\"\n",
    "\n",
    "for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]):\n",
    "    X = iris.data[:, pair]\n",
    "    y = iris.target\n",
    "\n",
    "    plt.subplot(3, 2, pairidx + 1)\n",
    "    plt.xlabel(iris.feature_names[pair[0]])\n",
    "    plt.ylabel(iris.feature_names[pair[1]])\n",
    "    plt.grid(True)\n",
    "        \n",
    "    plt.plot(X[y==0,0], X[y==0,1], 'o', label=iris.target_names[0], color='#993300')\n",
    "    plt.plot(X[y==1,0], X[y==1,1], 'o', label=iris.target_names[1], color='#009933')\n",
    "    plt.plot(X[y==2,0], X[y==2,1], 'o', label=iris.target_names[2], color='#000099')\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4seH-NF7BxYM"
   },
   "source": [
    "### <font color=\"#CA3532\">Preparación de los datos</font>\n",
    "\n",
    "Vamos a considerar sólo las dos últimas dimensiones del problema (la longitud y la anchura del pétalo). Esto nos permitirá visualizar el modelo en 2D. \n",
    "\n",
    "A continuación generamos los arrays de datos ``x`` y ``t``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dNRYb-1B0G2"
   },
   "outputs": [],
   "source": [
    "x = iris.data[:, -2:]\n",
    "t = iris.target\n",
    "[n, d] = x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UO7xvOqAXgOg"
   },
   "source": [
    "Los datos del problema están ordenados (los primeros 50 ejemplos son de la clase setosa, los 50 siguientes de la clase versicolor, y los 50 últimos de la clase virginica). Para evitar sesgos durante el entrenamiento del modelo conviene desordenarlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqQOBQknB0Da"
   },
   "outputs": [],
   "source": [
    "p = np.random.permutation(n)\n",
    "x = x[p, :]\n",
    "t = t[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfYEJNNtYHBA"
   },
   "source": [
    "Finalmente trasponemos la variable ``x`` para que tenga dimensiones ``2x150`` e introducimos una dimensión *dummy* en la variable ``t`` para que tenga dimensiones ``1x150``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryBkLPJ7B0AU"
   },
   "outputs": [],
   "source": [
    "x = x.T\n",
    "t = t[None, :]\n",
    "print(\"Dimension de x:\", x.shape)\n",
    "print(\"Dimension de t:\", t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkufW1Ov_UYY"
   },
   "source": [
    "La figura siguiente muestra los datos del problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDP11svS6Bk0"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.plot(x[0, t.ravel()==0], x[1, t.ravel()==0], 'o', label=iris.target_names[0], color='#993300')\n",
    "plt.plot(x[0, t.ravel()==1], x[1, t.ravel()==1], 'o', label=iris.target_names[1], color='#009933')\n",
    "plt.plot(x[0, t.ravel()==2], x[1, t.ravel()==2], 'o', label=iris.target_names[2], color='#000099')\n",
    "\n",
    "plt.xlabel(iris.feature_names[pair[0]])\n",
    "plt.ylabel(iris.feature_names[pair[1]])\n",
    "plt.grid(True)\n",
    "      \n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TkqclYo6GGmd"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1W-oLw8BF-n9"
   },
   "outputs": [],
   "source": [
    "# Particion entrenamiento-test:\n",
    "x_train, x_test, t_train, t_test = train_test_split(x.T, t.T, test_size=0.33, random_state=12)\n",
    "\n",
    "# Estandarizacion:\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJck1qlXGDPh"
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZeJ9hcABbTc"
   },
   "source": [
    "## <font color=\"#CA3532\">Estrategia One-vs-rest</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2qOzMovBm7C"
   },
   "source": [
    "Para aquellos modelos más sencillos, como la regresión logística que hemos visto, una alternativa para construir un modelo de decisión multiclase es utilizar la estrategia **One-vs-rest**.\n",
    "\n",
    "La estrategia consiste en construir N modelos para que cada uno aprenda a identificar cada una de las N clases por separado, diferenciándolas de los demás. Una vez los modelos se han entrenado **con los mismos hiperparámetros**, se normalizan las predicciones para dar la clase con mayor probabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mErTrMxzY9V8"
   },
   "source": [
    "### <font color=\"#CA3532\">Definición del modelo</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_kqtISVCXH6"
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionModel:\n",
    "\n",
    "  def __init__(self, d0=2):\n",
    "    self.W = tf.Variable(tf.random.normal(shape=[d0, 1], dtype=tf.dtypes.float64))  \n",
    "    self.b = tf.Variable(tf.random.normal(shape=[1], dtype=tf.dtypes.float64)) \n",
    "\n",
    "  def predict(self, x):\n",
    "    \"\"\"\n",
    "    x must be a (n,d0) array\n",
    "    returns a (n,1) array with the predictions for each of the n patterns\n",
    "    \"\"\"\n",
    "    z = tf.matmul(x, self.W) + self.b\n",
    "    y = tf.math.sigmoid(z)\n",
    "    return y\n",
    "\n",
    "  def loss(self, x, t):\n",
    "    \"\"\"\n",
    "    computes the cross-entropy between the model predictions and the targets\n",
    "    \"\"\"\n",
    "    y = self.predict(x)\n",
    "    loss = tf.reduce_mean(-t*tf.math.log(y) - (1.-t)*tf.math.log(1.-y), axis=0)\n",
    "    return loss\n",
    "\n",
    "  def fit(self, x, t, eta, num_epochs):\n",
    "    \"\"\"\n",
    "    Fits the model parameters with data (x, t) using a learning rate eta and\n",
    "    num_epochs epochs\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "      with tf.GradientTape() as tape:\n",
    "        loss = self.loss(x, t)\n",
    "\n",
    "      loss_history.append(loss.numpy().ravel()[0])\n",
    "      \n",
    "      [db, dW] = tape.gradient(loss, [self.b, self.W])\n",
    "      self.b.assign(self.b - eta*db)\n",
    "      self.W.assign(self.W - eta*dW)\n",
    "      \n",
    "    return loss_history\n",
    "\n",
    "  def accuracy(self, x, t):\n",
    "    y = self.predict(x).numpy()\n",
    "    pred = y > 0.5\n",
    "    return np.mean(pred == t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbIGVM4OOdXv"
   },
   "source": [
    "Una vez definida la clase *LogisticRegressionModel*, voy a definir una clase *MulticlassLogisticRegressionModel*. Esta clase tiene que hacer lo siguiente:\n",
    "\n",
    "- Tener un conjunto de modelos de tipo *LogisticRegressionModel*, tantos como clases diferentes a clasificar haya en el problema (en el ejemplo que estamos viendo, son 3 clases: setosa, versicolor y virginica).\n",
    "\n",
    "- Entrenar el modelo multiclase significa entrenar cada modelo con cada clase a pares, es decir, el modelo 0 debe entrenarse para diferenciar la clase 0 de las demás; el modelo 1 debe entrenarse para diferenciar la clase 1 de las demás; y el modelo 2 debe entrenarse para diferenciar la clase 2 de las demás.\n",
    "\n",
    "- Predecir el resultado final debe dar la probabilidad que la combinación de los tres modelos da a cada clase. Para ello, lo que vamos a hacer es normalizar las probabilidades siguiendo la siguiente ecuación:\n",
    "\n",
    "$$P(clase=i)(x) = \\frac{P_{modelo=i}(clase=i)(x)}{\\sum_{j=0}^{j=N\\_clases}{P_{modelo=j}(clase=j)(x)}}$$\n",
    "\n",
    "Donde $P_{modelo=i}(clase=i)(x)$ es la probabilidad que el modelo asociado a la clase $i$ da al dato $x$ de pertenecer a la clase $i$ frente a las demás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWd44334CfuA"
   },
   "outputs": [],
   "source": [
    "class MulticlassLogisticRegressionModel:\n",
    "\n",
    "  def __init__(self, input_dimension, num_classes):\n",
    "    self.models = [LogisticRegressionModel(input_dimension) for _ in range(num_classes)]\n",
    "    self.num_classes = num_classes\n",
    "\n",
    "  def fit(self, x, t, eta, num_epochs):\n",
    "    for id_class, model in enumerate(self.models):\n",
    "      t_class = (t == id_class) * 1\n",
    "      model.fit(x, t_class, eta, num_epochs)\n",
    "\n",
    "  def predict(self, x):\n",
    "    preds = np.zeros((x.shape[0], self.num_classes))\n",
    "    for id_class, model in enumerate(self.models):\n",
    "      preds[:, id_class] = model.predict(x)[:, 0]\n",
    "    preds /= preds.sum(axis=1, keepdims=True)\n",
    "    return preds\n",
    "\n",
    "  def accuracy(self, x, t):\n",
    "    preds = self.predict(x)\n",
    "    y = np.argmax(preds, axis=1)[:, None]\n",
    "    print(y.shape)\n",
    "    return np.mean(y == t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMXyWIccZEo4"
   },
   "source": [
    "### <font color=\"#CA3532\">Entrenamiento</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGlsVgScFeSw"
   },
   "outputs": [],
   "source": [
    "# Construccion del modelo\n",
    "n, d = x_train.shape\n",
    "num_clases = len(np.unique(t_train))\n",
    "\n",
    "model = MulticlassLogisticRegressionModel(d, num_clases)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "eta = 0.8\n",
    "epochs = 200\n",
    "\n",
    "model.fit(x_train, t_train, eta, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMcjrjUmGj2a"
   },
   "outputs": [],
   "source": [
    "preds_train = model.predict(x_train)\n",
    "preds_test = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1656750011661,
     "user": {
      "displayName": "christian oliva moya",
      "userId": "04396812984644697862"
     },
     "user_tz": -120
    },
    "id": "_E0pXCnfHdcR",
    "outputId": "b8401533-8498-4408-c983-26a3d4a405fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n",
      "Accuracy (train): 0.96\n",
      "(50, 1)\n",
      "Accuracy (test): 0.96\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy (train):\", model.accuracy(x_train, t_train))\n",
    "print(\"Accuracy (test):\", model.accuracy(x_test, t_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcrtWyj2ZLCN"
   },
   "source": [
    "### <font color=\"#CA3532\">Visualización del modelo</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOURkDEjRLHr"
   },
   "source": [
    "Pintamos la decisión global del modelo One-vs-rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JfqaapwKHXh6"
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.arange(x_train[:, 0].min()-0.2, x_train[:, 0].max()+0.2, 0.1), \n",
    "                     np.arange(x_train[:, 1].min()-0.2, x_train[:, 1].max()+0.2, 0.1))\n",
    "xy = np.concatenate([xx.reshape([1, -1]), yy.reshape([1, -1])],axis=0).T\n",
    "z = model.predict(xy)\n",
    "print(z.shape)\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "for clase_objetivo in [0, 1, 2]:\n",
    "  plt.subplot(1, 3, clase_objetivo+1)\n",
    "  plt.contourf(xx, yy, z[:, clase_objetivo].reshape(xx.shape), 100, cmap=\"bwr\", alpha=0.4, vmin=0.0, vmax=1.0)\n",
    "\n",
    "  plt.plot(x_test[t_test.ravel()==0, 0], x_test[t_test.ravel()==0, 1], 'o', label=iris.target_names[0] if clase_objetivo == 0 else 'others', color='red' if clase_objetivo == 0 else 'blue')\n",
    "  plt.plot(x_test[t_test.ravel()==1, 0], x_test[t_test.ravel()==1, 1], 'o', label=iris.target_names[1] if clase_objetivo == 1 else 'others', color='red' if clase_objetivo == 1 else 'blue')\n",
    "  plt.plot(x_test[t_test.ravel()==2, 0], x_test[t_test.ravel()==2, 1], 'o', label=iris.target_names[2] if clase_objetivo == 2 else 'others', color='red' if clase_objetivo == 2 else 'blue')\n",
    "\n",
    "  plt.title(\"Probabilidad de %s\" % (iris.target_names[clase_objetivo]))\n",
    "  plt.xlabel(iris.feature_names[pair[0]])\n",
    "  plt.ylabel(iris.feature_names[pair[1]])\n",
    "  plt.grid(True)\n",
    "        \n",
    "  plt.legend(loc=2)\n",
    "  plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdPoTYJ1ROdo"
   },
   "source": [
    "Ahora pintamos la frontera de decisión de cada uno de los modelos que componen el modelo global One-vs-rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TH88O3iqQPdo"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "for clase_objetivo, m in enumerate(model.models):\n",
    "  xx, yy = np.meshgrid(np.arange(x_train[:, 0].min()-0.2, x_train[:, 0].max()+0.2, 0.1), \n",
    "                     np.arange(x_train[:, 1].min()-0.2, x_train[:, 1].max()+0.2, 0.1))\n",
    "  xy = np.concatenate([xx.reshape([1, -1]), yy.reshape([1, -1])],axis=0).T\n",
    "  z = m.predict(xy).numpy()\n",
    "\n",
    "  plt.subplot(1, 3, clase_objetivo+1)\n",
    "  plt.contourf(xx, yy, z.reshape(xx.shape), 100, cmap=\"bwr\", alpha=0.4, vmin=0.0, vmax=1.0)\n",
    "\n",
    "  plt.plot(x_test[t_test.ravel()==0, 0], x_test[t_test.ravel()==0, 1], 'o', label=iris.target_names[0] if clase_objetivo == 0 else 'others', color='red' if clase_objetivo == 0 else 'blue')\n",
    "  plt.plot(x_test[t_test.ravel()==1, 0], x_test[t_test.ravel()==1, 1], 'o', label=iris.target_names[1] if clase_objetivo == 1 else 'others', color='red' if clase_objetivo == 1 else 'blue')\n",
    "  plt.plot(x_test[t_test.ravel()==2, 0], x_test[t_test.ravel()==2, 1], 'o', label=iris.target_names[2] if clase_objetivo == 2 else 'others', color='red' if clase_objetivo == 2 else 'blue')\n",
    "\n",
    "  plt.title(\"Probabilidad de %s\" % (iris.target_names[clase_objetivo]))\n",
    "  plt.xlabel(iris.feature_names[pair[0]])\n",
    "  plt.ylabel(iris.feature_names[pair[1]])\n",
    "  plt.grid(True)\n",
    "        \n",
    "  plt.legend(loc=2)\n",
    "  plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Abuu2otRBjOl"
   },
   "source": [
    "## <font color=\"#CA3532\">Modelo multiclase: Red neuronal</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5pcU8PIqBgk"
   },
   "source": [
    "### <font color=\"#CA3532\">Definición del modelo</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWq9FTCeR3PB"
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkModel:\n",
    "\n",
    "  def __init__(self, layers_size=[2]):\n",
    "    self.W = [tf.Variable(tf.random.normal(shape=[a, b], dtype=tf.dtypes.float64)) for a, b in zip(layers_size[:-1], layers_size[1:])]\n",
    "    self.b = [tf.Variable(tf.random.normal(shape=[1, b], dtype=tf.dtypes.float64)) for b in layers_size[1:]]\n",
    "\n",
    "  def predict_pre_activation(self, x):\n",
    "    \"\"\"\n",
    "    x must be a (n,d0) array\n",
    "    returns a (n,num_clases) array with the pre-activations for each of the n patterns\n",
    "    \"\"\"\n",
    "    y = x\n",
    "    for w, b in zip(self.W[:-1], self.b[:-1]):\n",
    "      z = tf.matmul(y, w) + b\n",
    "      y = tf.nn.sigmoid(z)\n",
    "\n",
    "    z = tf.matmul(y, self.W[-1]) + self.b[-1]\n",
    "\n",
    "    return z\n",
    "\n",
    "  def predict(self, x):\n",
    "    \"\"\"\n",
    "    x must be a (n,d0) array\n",
    "    returns a (n,num_clases) array with the predictions for each of the n patterns\n",
    "    \"\"\"\n",
    "    y = x\n",
    "    for w, b in zip(self.W[:-1], self.b[:-1]):\n",
    "      z = tf.matmul(y, w) + b\n",
    "      y = tf.nn.sigmoid(z)\n",
    "\n",
    "    z = tf.matmul(y, self.W[-1]) + self.b[-1]\n",
    "    y = tf.nn.softmax(z, axis=1)\n",
    " \n",
    "    return y\n",
    "\n",
    "  def loss(self, x, t):\n",
    "    \"\"\"\n",
    "    computes the MSE between the model predictions and the targets\n",
    "    \"\"\"\n",
    "    z = self.predict_pre_activation(x)\n",
    "    xentropy = tf.compat.v1.losses.sparse_softmax_cross_entropy(t, z) # Esta función necesita que pasemos los logits\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    return loss\n",
    "\n",
    "  def fit(self, x, t, eta, num_epochs):\n",
    "    \"\"\"\n",
    "    Fits the model parameters with data (x, t) using a learning rate eta and\n",
    "    num_epochs epochs\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "      with tf.GradientTape(persistent=True) as tape:\n",
    "        loss = self.loss(x, t)\n",
    "\n",
    "      loss_history.append(loss.numpy().ravel()[0])\n",
    "      \n",
    "      for b, W in zip(self.b, self.W):\n",
    "        [db, dW] = tape.gradient(loss, [b, W])\n",
    "        b.assign(b - eta*db)\n",
    "        W.assign(W - eta*dW)\n",
    "      \n",
    "    return loss_history\n",
    "\n",
    "  def accuracy(self, x, t):\n",
    "    preds = self.predict(x).numpy()\n",
    "    y = np.argmax(preds, axis=1)[:, None]\n",
    "    return np.mean(y == t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlNA0G7ewCya"
   },
   "source": [
    "### <font color=\"#CA3532\">Entrenamiento</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEgaoM9_ZBgL"
   },
   "outputs": [],
   "source": [
    "nepocas = 500\n",
    "eta = 0.1\n",
    "\n",
    "model = NeuralNetworkModel([2, 20, 3])\n",
    "\n",
    "loss = model.fit(x_train, t_train, eta, nepocas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbLBWVvC09vH"
   },
   "source": [
    "### <font color=\"#CA3532\">Coste frente a número de épocas</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SlQuynOhFiD"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"época\")\n",
    "plt.ylabel(\"Cross-entropy loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6dqjMfpVfeP"
   },
   "outputs": [],
   "source": [
    "preds_train = model.predict(x_train).numpy()\n",
    "preds_test = model.predict(x_test).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTids0CSVwaJ"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy (train):\", model.accuracy(x_train, t_train))\n",
    "print(\"Accuracy (test):\", model.accuracy(x_test, t_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRi6_5jKAHiE"
   },
   "source": [
    "### <font color=\"#CA3532\">Visualización del modelo</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQV191VL5Gb5"
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.arange(x_train[:, 0].min()-0.5, x_train[:, 0].max()+0.5, 0.1), \n",
    "                     np.arange(x_train[:, 1].min()-0.5, x_train[:, 1].max()+0.5, 0.1))\n",
    "xy = np.concatenate([xx.reshape([1, -1]), yy.reshape([1, -1])],axis=0).T\n",
    "z = model.predict(xy).numpy()\n",
    "print(z.shape)\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "for clase_objetivo in [0, 1, 2]:\n",
    "  plt.subplot(1, 3, clase_objetivo+1)\n",
    "  plt.contourf(xx, yy, z[:, clase_objetivo].reshape(xx.shape), 100, cmap=\"bwr\", alpha=0.4, vmin=0.0, vmax=1.0)\n",
    "\n",
    "  plt.plot(x_test[t_test.ravel()==0, 0], x_test[t_test.ravel()==0, 1], 'o', label=iris.target_names[0] if clase_objetivo == 0 else 'others', color='red' if clase_objetivo == 0 else 'blue')\n",
    "  plt.plot(x_test[t_test.ravel()==1, 0], x_test[t_test.ravel()==1, 1], 'o', label=iris.target_names[1] if clase_objetivo == 1 else 'others', color='red' if clase_objetivo == 1 else 'blue')\n",
    "  plt.plot(x_test[t_test.ravel()==2, 0], x_test[t_test.ravel()==2, 1], 'o', label=iris.target_names[2] if clase_objetivo == 2 else 'others', color='red' if clase_objetivo == 2 else 'blue')\n",
    "\n",
    "  plt.title(\"Probabilidad de %s\" % (iris.target_names[clase_objetivo]))\n",
    "  plt.xlabel(iris.feature_names[pair[0]])\n",
    "  plt.ylabel(iris.feature_names[pair[1]])\n",
    "  plt.grid(True)\n",
    "        \n",
    "  plt.legend(loc=2)\n",
    "  plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z11tuiOu3OXx"
   },
   "source": [
    "## <font color=\"#CA3532\">Ejercicios adicionales</font>\n",
    "\n",
    "- Calcular matriz de confusión\n",
    "- Análisis ROC\n",
    "- Precision-recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fo0xGEXS2_P3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mV3hvjTh4K5V"
   },
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(t_test[:, 0], np.argmax(preds_test, axis=1))\n",
    "\n",
    "sn.heatmap(matrix, annot=True, xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSU3SZWeb_x_"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "for i in range(3):\n",
    "  fpr, tpr, thresholds = roc_curve(t_test[:, 0] == i, np.argmax(preds_test, axis=1) == i)\n",
    "  plt.subplot(1,3,i+1)\n",
    "  plt.plot(fpr, tpr)\n",
    "  plt.xlabel(\"False Positive Rate\")\n",
    "  plt.ylabel(\"True Positive Rate\")\n",
    "  plt.title(\"ROC Curve - Class \" + iris.target_names[i])\n",
    "  plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMDEiwBkd-X0"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "for i in range(3):\n",
    "  prec, recall, _ = precision_recall_curve(t_test[:, 0] == i, np.argmax(preds_test, axis=1) == i)\n",
    "  print(recall, prec)\n",
    "  plt.subplot(1,3,i+1)\n",
    "  plt.plot(recall, prec)\n",
    "  plt.xlabel(\"Recall\")\n",
    "  plt.ylabel(\"Precision\")\n",
    "  plt.title(\"Precision-Recall Curve - Class \" + iris.target_names[i])\n",
    "  plt.ylim(-0.05, 1.05)\n",
    "  plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DA81DHfefQc4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
