{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_uxTWnlyd1Q"
   },
   "source": [
    "<img>\n",
    "<font color=\"#CA3532\"><h1 align=\"left\">Deep Learning</h1></font>\n",
    "<font color=\"#6E6E6E\"><h2 align=\"left\">Introducción a Keras - Parte 2</h2></font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9qWoPVLyd1T"
   },
   "source": [
    "# <font color=\"#CA3532\">Resolviendo MNIST con Keras</font>\n",
    "\n",
    "En este notebook vamos a construir una red neuronal para el problema MNIST (http://yann.lecun.com/exdb/mnist/) usando Keras. Lo primero, como siempre, es importar las librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxL8U9Jdyd1U",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdvFKkKvyd1Z"
   },
   "source": [
    "Cargamos los datos de MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uY4N7AEZyd1a"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(train_labels)\n",
    "\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijpLDkmTyd1g"
   },
   "source": [
    "Dibujamos algunas de las imágenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmCTexh5yd1h"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(train_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKqwpZG8yd1l"
   },
   "source": [
    "Antes de construir los modelos normalizamos las imágenes dividiendo entre el valor máximo para tenerlo entre 0 y 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFR_Um5Vyd1m"
   },
   "outputs": [],
   "source": [
    "train_images = train_images / 255\n",
    "test_images = test_images / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFA6RI2_ntBt"
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0], cmap=plt.cm.binary)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O31bdxH3ng3v"
   },
   "source": [
    "Ahora restamos la media:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhqefaaQndAy"
   },
   "outputs": [],
   "source": [
    "mean_img = train_images.mean(axis=0)\n",
    "train_images = train_images - mean_img\n",
    "test_images = test_images - mean_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ax-Jtw2N6SX2"
   },
   "outputs": [],
   "source": [
    "plt.imshow(mean_img, cmap=plt.cm.binary)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uhg59kHm6gFt"
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0], cmap='bwr', vmin=-1, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Q8KpxLzyd4O"
   },
   "source": [
    "## <font color=\"#CA3532\">Ejercicio</color>\n",
    "\n",
    "Probar con diferentes hiperparámetros para MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YY1Nsie8enk9"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZDxVnEPHqoX"
   },
   "outputs": [],
   "source": [
    "# Borramos logs para visualizar solamente nuestros modelos\n",
    "%cd /content/drive/MyDrive/\n",
    "!rm -rf logs_keras_miax10_parte2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MLsgMnKvp1S"
   },
   "outputs": [],
   "source": [
    "# Variables que no vamos a modificar\n",
    "log_dir = \"/content/drive/MyDrive/logs_keras_miax10_parte2/\"\n",
    "input_shape = (28, 28)\n",
    "num_clases = 10\n",
    "n_epochs = 20\n",
    "\n",
    "LEARNING_RATE_BASE = 0.01\n",
    "BATCH_SIZE_BASE = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLBQDPL1rBw3"
   },
   "source": [
    "### <font color=\"#CA3532\">Modelo base</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VoPRclqBrDNW"
   },
   "outputs": [],
   "source": [
    "# Caso base:\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'sigmoid'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'base'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_iPJTksZ0FM"
   },
   "source": [
    "### <font color=\"#CA3532\">Inicialización de pesos</font>\n",
    "\n",
    "Documentación de Keras: https://keras.io/api/layers/initializers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M-kb3l2255n"
   },
   "source": [
    "La inicialización de pesos a ceros se hace con el inicializador ``tf.keras.initializers.Zeros()``. Hemos visto en teoría que inicializar los pesos a 0 no es nada eficiente. Vamos a comprobarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrP8cwb8Ks5k"
   },
   "outputs": [],
   "source": [
    "# Caso inicialización de pesos a 0:\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'sigmoid'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'zeros'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.Zeros())) # Capa densa inicializada a 0s\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.Zeros())) # Capa softmax inicializada a 0s\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmbhDH3G36m1"
   },
   "source": [
    "La inicialización de pesos con una Normal se hace con el inicializador ``tf.keras.initializers.RandomNormal(mean, stddev)``. Hemos visto en teoría que inicializar los pesos con valores cercanos a 0 es mucho más eficaz que inicializarlos con valores mayores. Vamos a probarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDKcm4RY37df"
   },
   "outputs": [],
   "source": [
    "# Caso Normal con std pequeña:\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'sigmoid'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'normal_close_to_0'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.05))) # Capa densa inicializada a Normal(0, 0.05)\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.05))) # Capa densa inicializada a Normal(0, 0.05)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMVhH8PY4j7y"
   },
   "outputs": [],
   "source": [
    "# Caso Normal con std grande:\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'sigmoid'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'normal_far_from_0'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0, 1.0)\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0, 1.0)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdXFbbds4yAv"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzqlXwDd5urU"
   },
   "outputs": [],
   "source": [
    "# Caso Normal con std estimada:\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'sigmoid'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'normal_good_std'\n",
    "\n",
    "std = 1 / np.sqrt(train_images.shape[1])\n",
    "print(\" > STD estimada:\", std)\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=std))) # Capa densa inicializada a Normal(0, std)\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0, 1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lhZvDb56E0H"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MjVmWLL6Ry5"
   },
   "source": [
    "Como vemos en los experimentos, inicializar los pesos de forma apropiada es fundamental para tener un buen entrenamiento. Vamos a comparar la Normal con la std calculada con otros inicializadores más complejos, como **HeNormal**, **GlorotNormal** o **GlorotUniform**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZU8vLjqKlPx"
   },
   "source": [
    "HeNormal prácticamente es equivalente a la inicialización buscando la mejor std que hemos calculado previamente. Define la desviación estándar mediante la siguiente fórmula:\n",
    "\n",
    "$$stddev = \\sqrt{\\frac{2}{N_{input}}}$$\n",
    "\n",
    "Keras API: https://keras.io/api/layers/initializers/#henormal-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55DHnDzY64CX"
   },
   "outputs": [],
   "source": [
    "# Caso HeNormal:\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'sigmoid'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'heNormal'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.HeNormal())) # Capa densa inicializada a HeNormal()\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0, 1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGrvNiGxLE1p"
   },
   "source": [
    "GlorotNormal o XavierNormal define la desviación estándar mediante la siguiente fórmula:\n",
    "\n",
    "$$stddev = \\sqrt{\\frac{2}{N_{input} + N_{output}}}$$\n",
    "\n",
    "Keras API: https://keras.io/api/layers/initializers/#glorotnormal-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4afsO857Ktr"
   },
   "outputs": [],
   "source": [
    "# Caso GlorotNormal:\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'sigmoid'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'glorotNormal'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.GlorotNormal())) # Capa densa inicializada a GlorotNormal()\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0, 1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9i8yTCsLTkk"
   },
   "source": [
    "GlorotUniform o XavierUniform define la inicialización de pesos de manera uniforme en el rango de valores ```[-limit, limit]``` donde:\n",
    "\n",
    "$$limit = \\sqrt{\\frac{6}{N_{input} + N_{output}}}$$\n",
    "\n",
    "Keras API: https://keras.io/api/layers/initializers/#glorotuniform-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iK_hnZPp7Zzb"
   },
   "outputs": [],
   "source": [
    "# Caso GlorotUniform:\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'sigmoid'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'glorotUniform'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.GlorotUniform())) # Capa densa inicializada a GlorotUniform()\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0, 1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VF4EpPBI7j5i"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kOmP7Ax80gJ"
   },
   "source": [
    "La selección de la inicialización de los pesos es crucial en algunos problemas. Además, es muy sensible a las distintas activaciones que puede tener la capa. Vamos a probar la inicialización con una activación RELU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFSVykSzOerk"
   },
   "outputs": [],
   "source": [
    "# Caso Base RELU:\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'relu'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'base-relu'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UYzshS8n9Xjn"
   },
   "outputs": [],
   "source": [
    "# Caso RELU HeNormal:\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'relu'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'relu-heNormal'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.HeNormal())) # Capa densa inicializada a HeNormal()\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0,1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQyI2DXq9kC3"
   },
   "outputs": [],
   "source": [
    "# Caso RELU GlorotNormal:\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'relu'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'relu-glorotNormal'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.GlorotNormal())) # Capa densa inicializada a GlorotNormal()\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0,1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cK07W9BA9smZ"
   },
   "outputs": [],
   "source": [
    "# Caso RELU GlorotUniform:\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'relu'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'relu-glorotUniform'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.GlorotUniform())) # Capa densa inicializada a GlorotUniform()\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0,1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHOXfsLR9x_k"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GH5r0pEOK6D"
   },
   "source": [
    "**Preguntas**\n",
    "\n",
    "* ¿Es tan importante la inicialización de la capa de salida a N(0, 1)?\n",
    "\n",
    "* ¿Por qué los días anteriores estaba funcionando tan bien la inicialización de la capa Densa si no le decíamos nada?\n",
    "\n",
    "Keras API: https://keras.io/api/layers/core_layers/dense/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBlmOBivZ6Az"
   },
   "source": [
    "### <font color=\"#CA3532\">Batch normalization</font>\n",
    "\n",
    "El objetivo de utilizar Batch Normalization simplifica la tarea de inicializar los pesos, ya que la red no es tan sensible a una mala inicialización. Vamos a probarlo con una mala inicialización **Normal(0,10)** para las funciones de activación sigmoid y RELU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mExLbABIRlpi"
   },
   "outputs": [],
   "source": [
    "# Caso Normal(0,0.01):\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'sigmoid'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'malaInicializacion'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01))) # Capa densa inicializada a Normal(0,0.01)\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01))) # Capa densa inicializada a Normal(0,0.01)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKriqnm1-92O"
   },
   "outputs": [],
   "source": [
    "# Caso Normal(0,0.01) con BATCH NORMALIZATION:\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'sigmoid'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'malaInicializacion-batchNormalization'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=None, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01))) # Capa densa inicializada a Normal(0,0.01)\n",
    "model.add(tf.keras.layers.BatchNormalization()) # Capa batch normalization\n",
    "model.add(keras.layers.Activation(activation)) # Aplicamos la activación después de aplicar el batch normalization\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01))) # Capa densa inicializada a Normal(0,0.01)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7IjSA0SP2in"
   },
   "outputs": [],
   "source": [
    "# Caso RELU Normal(0,0.01):\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'relu'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'relu-malaInicializacion'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01))) # Capa densa inicializada a Normal(0,0.01)\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01))) # Capa densa inicializada a Normal(0,0.01)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnnguvgn_hbH"
   },
   "outputs": [],
   "source": [
    "# Caso RELU Normal(0,0.01) con BATCH NORMALIZATION:\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'relu'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'relu-malaInicializacion-batchNormalization'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=None, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01))) # Capa densa inicializada a Normal(0,0.01)\n",
    "model.add(tf.keras.layers.BatchNormalization()) # Capa batch normalization\n",
    "model.add(keras.layers.Activation(activation)) # Aplicamos la activación después de aplicar el batch normalization\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01))) # Capa densa inicializada a Normal(0,0.01)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0qVhDg2_xRc"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pKhdnHQSQSE"
   },
   "source": [
    "Con BatchNormalization, los pesos no necesariamente tienen que estar bien ajustados. Si ponemos pesos cerca de 0 (*Normal(0, 0.01)*), que es una mala inicialización, BatchNormalization hace que el entrenamiento vaya mucho mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7RPKUgzaK3y"
   },
   "source": [
    "### <font color=\"#CA3532\">Optimizadores</font>\n",
    "\n",
    "Hasta ahora, hemos estado trabajando siempre con el optimizador de descenso por gradiente estándar (SGD). Sin embargo, hemos visto en la parte de teoría que existen diferentes algoritmos de optimización. Vamos a probarlos con el modelo con activación RELU inicializado de la forma más óptima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2GhuxSWBb9m"
   },
   "outputs": [],
   "source": [
    "# Caso RELU GlorotNormal con BatchNormalization y SGD\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'relu'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'sgd'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=None, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.GlorotNormal())) # Capa densa inicializada a GlorotNormal()\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(activation))\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0, 1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfXOlai6BzYK"
   },
   "outputs": [],
   "source": [
    "# Caso RELU GlorotNormal con BatchNormalization y MOMENTO\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'relu'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'momento'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=None, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.GlorotNormal())) # Capa densa inicializada a GlorotNormal()\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(activation))\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0, 1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.8), ### Se añade el argumento momentum al SGD\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byDI3BQNCGVf"
   },
   "outputs": [],
   "source": [
    "# Caso RELU Normal con std estimada y MOMENTO Nesterov\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'relu'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'nesterov'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=None, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.GlorotNormal())) # Capa densa inicializada a GlorotNormal()\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(activation))\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0, 1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.8, nesterov=True), ### Se añade el argumento nesterov = True\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylyuWSCACTwA"
   },
   "outputs": [],
   "source": [
    "# Caso RELU Normal con std estimada y ADAGRAD\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'relu'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'adagrad'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=None, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.GlorotNormal())) # Capa densa inicializada a GlorotNormal()\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(activation))\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0, 1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adagrad(learning_rate=learning_rate), ### Cambiamos SGD por ADAGRAD\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01gWCiF9CqOh"
   },
   "outputs": [],
   "source": [
    "# Caso RELU Normal con std estimada y RMSPROP\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'relu'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'rmsprop'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=None, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.GlorotNormal())) # Capa densa inicializada a GlorotNormal()\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(activation))\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0, 1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate), ### Cambiamos SGD por RMSprop\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xu3pCDjOC2eG"
   },
   "outputs": [],
   "source": [
    "# Caso RELU Normal con std estimada y ADAM\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'relu'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'adam'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=None, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.GlorotNormal())) # Capa densa inicializada a GlorotNormal()\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(activation))\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0, 1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), ### Cambiamos SGD por Adam\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckOeNzvyC8nE"
   },
   "outputs": [],
   "source": [
    "# Caso RELU Normal con std estimada y N-ADAM (Adam con Nesterov Momentum)\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'relu'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'nadam'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=None, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.GlorotNormal())) # Capa densa inicializada a GlorotNormal()\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(activation))\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0, 1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Nadam(learning_rate=learning_rate), ### Cambiamos SGD por Nadam\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghBlaKl7DJJH"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbLsihoLaTHq"
   },
   "source": [
    "# <font color=\"#CA3532\">Selección óptima de hiperparámetros con un conjunto de validación</font>\n",
    "\n",
    "Hemos visto que hay muchos hiperparámetros a estimar, por lo que no podemos estar buscando manualmente cuáles son los más apropiados. Por un lado, necesitamos **evitar este proceso manual** y, por otro lado, necesitamos un **conjunto de validación** adicional (disjunto del conjunto de entrenamiento) para poder seleccionarlos.\n",
    "\n",
    "Respecto al conjunto de validación, tenemos varias alternativas:\n",
    "\n",
    "- Validación simple\n",
    "\n",
    "- Validación cruzada\n",
    "\n",
    "Respecto a la búsqueda de hiperparámetros, tenemos varias alternativas:\n",
    "\n",
    "- Búsqueda a fuerza bruta (GridSearch)\n",
    "\n",
    "- Búsqueda automática (Keras-tuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cz3_ZBoSGpcb"
   },
   "source": [
    "### <font color=\"#CA3532\">Validación simple</font>\n",
    "\n",
    "Una validación simple consiste en realizar la búsqueda de los hiperparámetros sobre este conjunto, es decir, seleccionar el modelo que obtiene el mejor resultado (mejor accuracy, por ejemplo) sobre el conjunto de validación. Vamos a generar un conjunto de validación de ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "705Sh3P_K0zS"
   },
   "outputs": [],
   "source": [
    "## Tenemos un dataset y lo dividimos en training y test\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "## NORMALIZACION\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255\n",
    "mean_img = train_images.mean(axis=0)\n",
    "train_images = train_images - mean_img\n",
    "test_images = test_images - mean_img\n",
    "\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(train_labels)\n",
    "print()\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fql0SIP4Hj6L"
   },
   "outputs": [],
   "source": [
    "## Para generar un conjunto de validación, necesitamos dividir TRAINING en dos \n",
    "## subconjuntos. Podemos utilizar la función train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_images, validation_images, train_labels, validation_labels = train_test_split(train_images, train_labels, test_size=0.2)\n",
    "\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(train_labels)\n",
    "print()\n",
    "print(validation_images.shape)\n",
    "print(validation_labels.shape)\n",
    "print(validation_labels)\n",
    "print()\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqGd0ZrIK00X"
   },
   "outputs": [],
   "source": [
    "# Caso RELU GlorotNormal con BatchNormalization y ADAM (Adam)\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'relu'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'adam-validation'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=None, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.GlorotNormal())) # Capa densa inicializada a GlorotNormal()\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(activation))\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0, 1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), ### Adam\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo:\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(validation_images, validation_labels),\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX0nPHq9WHp7"
   },
   "source": [
    "En la celda previa, deberíamos buscar qué conjunto de hiperparámetros utilizar para conseguir el mayor accuracy en validación. Asumimos que ya los tenemos, por lo que es necesario reentrenar un modelo nuevo con todos los datos de training para evaluar en test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Vt745bBWeZj"
   },
   "outputs": [],
   "source": [
    "# Concatenamos datos de train y validacion\n",
    "final_train_images = np.concatenate((train_images, validation_images), axis=0)\n",
    "final_train_labels = np.concatenate((train_labels, validation_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v847A5WAWddp"
   },
   "outputs": [],
   "source": [
    "# Caso RELU GlorotNormal con BatchNormalization y ADAM (Adam)\n",
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'relu'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'adam-test'\n",
    "\n",
    "# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=None, name=\"oculta\",\n",
    "                             kernel_initializer=tf.keras.initializers.GlorotNormal())) # Capa densa inicializada a GlorotNormal()\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(activation))\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                             kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0, 1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), ### Adam\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Callback a TensorBoard:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n",
    "\n",
    "# Entrenamiento del modelo con los datos concatenados. OJO: NO HAY VALIDATION_DATA\n",
    "history = model.fit(final_train_images, \n",
    "                    final_train_labels, \n",
    "                    epochs=n_epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ceffc3M7K01h"
   },
   "outputs": [],
   "source": [
    "print(\" > Training:\", model.evaluate(final_train_images, final_train_labels))\n",
    "print(\" > Test:\", model.evaluate(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjbW47DRMctB"
   },
   "source": [
    "### <font color=\"#CA3532\">Validación cruzada</font>\n",
    "\n",
    "Una validación cruzada consiste en realizar la búsqueda de los hiperparámetros sobre un KFold, es decir, realizar K particiones del conjunto de entrenamiento (disjuntas) y calcular el promedio de accuracies utilizando K-1 particiones para training y 1 para validación. Así con todas las combinaciones posibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-xJgOUHPSsP"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "# kfold.split(...) devuelve un generador que genera los índices de entrenamiento\n",
    "# y validación para cada una de las particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AH3o6UxZPYQc"
   },
   "outputs": [],
   "source": [
    "## Tenemos un dataset y lo dividimos en training y test\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "## NORMALIZACION\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255\n",
    "mean_img = train_images.mean(axis=0)\n",
    "train_images = train_images - mean_img\n",
    "test_images = test_images - mean_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sqvz7cIUNH_h"
   },
   "outputs": [],
   "source": [
    "batch_size = BATCH_SIZE_BASE\n",
    "learning_rate = LEARNING_RATE_BASE\n",
    "activation = 'relu'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "nombre = 'adam-crossvalidation'\n",
    "\n",
    "cvscores = []\n",
    "for itrain, ival in kfold.split(train_images, train_labels):\n",
    "\n",
    "    # Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "    model.add(keras.layers.Dense(64, activation=None, name=\"oculta\",\n",
    "                                kernel_initializer=tf.keras.initializers.GlorotNormal())) # Capa densa inicializada a GlorotNormal()\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(activation))\n",
    "    model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                                kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0, 1)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), ### Cambiamos SGD por Adam\n",
    "                  loss=loss,\n",
    "                  metrics=['acc'])\n",
    "        \n",
    "    # Lo entrenamos:\n",
    "    history = model.fit(train_images[itrain], \n",
    "                        train_labels[itrain], \n",
    "                        epochs=n_epochs,\n",
    "                        verbose=0,\n",
    "                        batch_size=batch_size)\n",
    "    \n",
    "    # Lo evaluamos:\n",
    "    _, acc = model.evaluate(train_images[ival], train_labels[ival], verbose=0)\n",
    "    \n",
    "    print(\"Accuracy: %.2f%%\" % (acc*100.0))\n",
    "    cvscores.append(acc*100.0)\n",
    "print(\"%.2f%% \\u00B1 %.2f%%\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LD0UEj3NSoO5"
   },
   "outputs": [],
   "source": [
    "## Una vez has decidido los hiperparámetros que hacen máximo el valor del accuracy \n",
    "## del cross-val, hay que entrenar un nuevo modelo con esos hiperparámetros, esta\n",
    "## vez utilizando el 100% de los datos para entrenar.\n",
    "##\n",
    "## En esta situación no es necesario concatenar train y validación, ya que kfold.split\n",
    "## nos devolvía los índices del split, no un subconjunto de datos\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n",
    "model.add(keras.layers.Dense(64, activation=None, name=\"oculta\",\n",
    "                            kernel_initializer=tf.keras.initializers.GlorotNormal())) # Capa densa inicializada a GlorotNormal()\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(activation))\n",
    "model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\",\n",
    "                            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.0))) # Capa densa inicializada a Normal(0, 1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), ### Cambiamos SGD por Adam\n",
    "              loss=loss,\n",
    "              metrics=['acc'])\n",
    "    \n",
    "# Lo entrenamos con todos los datos de train. OJO: NO HAY VALIDATION_DATA\n",
    "history = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    epochs=n_epochs,\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWkImqxETl2k"
   },
   "outputs": [],
   "source": [
    "print(\" > Training:\", model.evaluate(train_images, train_labels))\n",
    "print(\" > Test:\", model.evaluate(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP2Ix2o-YXmL"
   },
   "source": [
    "En esta sección hemos visto como hacer el split de validación simple y validación cruzada. Sin embargo, no hemos realizado una búsqueda de hiperparámetros. Vamos a verlo con el problema de identificación de cáncer de mama, que es un dataset más pequeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhIzWTI_J0Ge"
   },
   "source": [
    "# <font color=\"#CA3532\">Resolviendo Breast Cancer con Keras</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjbWYSDIKiDr"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "x = data.data\n",
    "t = data.target[:, None]\n",
    "\n",
    "print(x.shape)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K55SsmZqKpTa"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, t_train, t_test = train_test_split(x, t, test_size=0.33, random_state=42)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BscPzsd2VAZW"
   },
   "source": [
    "## <font color=\"#CA3532\">GridSearch</font>\n",
    "\n",
    "GridSearch consiste en realizar una búsqueda por fuerza bruta probando todos los posibles valores en el rango especificado. Ahora vamos a programar a mano la búsqueda en gridSearch con el siguiente objetivo: realizar una validación cruzada utilizando una métrica de evaluación distinta: **F1-score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaolotARCFeb"
   },
   "outputs": [],
   "source": [
    "# Definimos el Kfold que vamos a utilizar en validación cruzada\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4GFY98y3ddR"
   },
   "outputs": [],
   "source": [
    "# Definimos la lista de hiperparametros que queremos buscar\n",
    "\n",
    "lista_numUnits = [32, 64]\n",
    "lista_learningRate = [0.01, 0.03, 0.1, 0.3]\n",
    "lista_batchSize = [500, 5000]\n",
    "lista_activations = ['tanh', 'relu', 'selu']\n",
    "lista_regularizationL2 = [0.0, 0.0001, 0.001]\n",
    "lista_dropout = [0.0, 0.2, 0.4]\n",
    "lista_inicializacion = ['Normal', 'GlorotNormal']\n",
    "lista_optimizadores = ['SGD', 'Adam', 'RMSprop']\n",
    "lista_losses = ['categorical_crossentropy', 'categorical_hinge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnEOW1CuAXg8"
   },
   "outputs": [],
   "source": [
    "## Transformamos los labels a categoricos para utilizar categorical_crossentropy y categorical_hinge\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "t_train_categorical = to_categorical(t_train)\n",
    "t_test_categorical = to_categorical(t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zn_VZZe-9-YM"
   },
   "outputs": [],
   "source": [
    "# Funcion de construcción del modelo que modifica algunas variables según sus valores iniciales\n",
    "#   initializer Normal hace el cálculo de la stddev óptima\n",
    "#   los optimizers deben crearse con el valor de learningRate\n",
    "#   si loss == categorical_hinge entonces la función de activación de la ultima capa debe ser None (lineal)\n",
    "\n",
    "def build_model(learningRate, activation, l2reg, dropout, initializer, optimizer, numUnits, loss, seed=1, metrics=['acc']):\n",
    "    if initializer == 'Normal':\n",
    "      stddev = 1 / np.sqrt(train_images.shape[1])\n",
    "      initializer = tf.keras.initializers.RandomNormal(stddev=stddev, seed=seed)\n",
    "    else: # GlorotNormal\n",
    "      initializer = tf.keras.initializers.GlorotNormal(seed=seed)\n",
    "\n",
    "    if optimizer == 'SGD':\n",
    "      optimizer = tf.keras.optimizers.SGD(learning_rate=learningRate)\n",
    "    elif optimizer == 'Adam':\n",
    "      optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate)\n",
    "    else: # 'RMSprop'\n",
    "      optimizer = tf.keras.optimizers.RMSprop(learning_rate=learningRate)\n",
    "\n",
    "    if loss == 'categorical_crossentropy':\n",
    "      output_activation = 'softmax'\n",
    "    else: # 'categorical_hinge'\n",
    "      output_activation = None\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Input(shape=(30)))\n",
    "    model.add(keras.layers.Dense(numUnits, activation=activation, name=\"oculta\",\n",
    "                                 kernel_initializer=initializer,\n",
    "                                 kernel_regularizer=tf.keras.regularizers.l2(l2reg)))\n",
    "    model.add(keras.layers.Dropout(dropout))\n",
    "    model.add(keras.layers.Dense(2, activation=output_activation, name=\"salida\", # 2 neuronas de salida porque es categorical [0, 1] o [1, 0]\n",
    "                                 kernel_initializer=initializer,\n",
    "                                 kernel_regularizer=tf.keras.regularizers.l2(l2reg)))\n",
    "\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss,\n",
    "                  metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yh5ILkejD85P"
   },
   "source": [
    "**Itertools** es una librería de python que te calcula fácilmente el producto cartesiano de un conjunto de listas utilizando la función ``itertools.product()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZjI_XRi9bqJ"
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdWEnD4O7VBS"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Como lo que se construye es un iterador, es necesario volver a ejecutarlo cada vez\n",
    "# que se vaya a utilizar\n",
    "combinations = itertools.product(lista_learningRate, lista_batchSize, lista_activations, \n",
    "                                 lista_regularizationL2, lista_dropout, lista_inicializacion,\n",
    "                                 lista_optimizadores, lista_numUnits, lista_losses)\n",
    "\n",
    "n_epochs = 20\n",
    "\n",
    "max_score = 0.0\n",
    "for learningRate, batchSize, activation, l2reg, dropout, initializer, optimizer, numUnits, loss in combinations:\n",
    "\n",
    "  # Código para validación cruzada utilizando kfold\n",
    "  cvscores = []\n",
    "  for itrain, ival in kfold.split(x_train, t_train):\n",
    "    model = build_model(learningRate, activation, l2reg, dropout, initializer, optimizer, numUnits, loss)\n",
    "    model.fit(x_train[itrain], \n",
    "              t_train_categorical[itrain], \n",
    "              epochs=n_epochs,\n",
    "              batch_size=batchSize,\n",
    "              verbose=0)\n",
    "    # Lo evaluamos:\n",
    "    _, acc = model.evaluate(x_train[ival], t_train_categorical[ival], verbose=0)\n",
    "    cvscores.append(acc*100.0)\n",
    "  \n",
    "  if np.mean(cvscores) > max_score:\n",
    "    best_config = (learningRate, batchSize, activation, l2reg, dropout, initializer, optimizer, numUnits, loss)\n",
    "    max_score = np.mean(cvscores)\n",
    "    print(\" > NEW Best config:\", best_config)\n",
    "\n",
    "print(\" > Best validation config:\", best_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxMNSlORcBQ0"
   },
   "outputs": [],
   "source": [
    "# Esta ejecución tarda muchísimo. El producto cartesiano son más de 5000 configuraciones diferentes\n",
    "# O bien tenemos mucho tiempo para dejar las máquinas ejecutando (en paralelo si es posible) o\n",
    "# tardará quizás días en realizar esta búsqueda por gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OazoIU7fE2sO"
   },
   "source": [
    "**Pregunta**: ¿Qué podríamos hacer para optimizar la búsqueda de hiperparámetros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UhEuxFicfNZ"
   },
   "outputs": [],
   "source": [
    "# Aquí entraría en juego nuestra \"intuición\". No es necesario probar todas las combinaciones\n",
    "# posibles si previamente hemos jugado con el problema y hemos visto que ciertas configuraciones\n",
    "# funcionan mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxL9UcZZKHl-"
   },
   "source": [
    "### <font color=\"#CA3532\">Ejercicio</font>\n",
    "\n",
    "Vamos a hacer una búsqueda paramétrica de solamente tres variables para calcular el modelo que mejor **F1-score** obtiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQ_s1x1nE1Vx"
   },
   "outputs": [],
   "source": [
    "# Definimos la lista de hiperparametros que queremos buscar\n",
    "\n",
    "lista_learningRate = [0.001, 0.01, 0.1]\n",
    "lista_activations = ['sigmoid', 'relu']\n",
    "lista_dropout = [0.0, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u08KzMkaFh9i"
   },
   "outputs": [],
   "source": [
    "combinations = itertools.product(lista_learningRate, lista_activations, lista_dropout)\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "counter = 0\n",
    "max_score = 0.0\n",
    "for learningRate, activation, dropout in combinations:\n",
    "  config = (learningRate, activation, dropout)\n",
    "  print(\" > Probando config:\", config)\n",
    "\n",
    "  # Código para validación cruzada utilizando kfold\n",
    "  cvscores = []\n",
    "  for itrain, ival in kfold.split(x_train, t_train):\n",
    "    model = build_model(learningRate, activation, 0.0, dropout, 'Normal', 'Adam', 20, 'categorical_crossentropy', metrics=['acc', 'Precision', 'Recall'])\n",
    "    model.fit(x_train[itrain], \n",
    "              t_train_categorical[itrain], \n",
    "              epochs=n_epochs,\n",
    "              batch_size=batch_size,\n",
    "              verbose=0)\n",
    "    # Lo evaluamos:\n",
    "    _, acc, prec, recall = model.evaluate(x_train[ival], t_train_categorical[ival], verbose=0)\n",
    "    f1_score = 2 * prec * recall / (prec + recall + 1e-8) # Añado un épsilon para evitar división entre 0\n",
    "    cvscores.append(f1_score)\n",
    "\n",
    "  print(\"   > Score:\", np.mean(cvscores))\n",
    "  \n",
    "  if np.mean(cvscores) > max_score:\n",
    "    best_config = config\n",
    "    max_score = np.mean(cvscores)\n",
    "    print(\"   >>> NEW Best config (\", max_score, \"):\", best_config)\n",
    "\n",
    "print(\"\\n > Best validation config (\", max_score, \"):\", best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmUmktW7eBAx"
   },
   "source": [
    "Ya tenemos la mejor configuración. Ahora entrenamos el modelo con los datos de train completos y evaluamos en test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OI9lA6QzeAMY"
   },
   "outputs": [],
   "source": [
    "# Fijo los hiperparámetros que he encontrado\n",
    "learningRate = 0.01\n",
    "activation = 'relu'\n",
    "dropout = 0.2\n",
    "\n",
    "model = build_model(learningRate, activation, 0.0, dropout, 'Normal', 'Adam', 20, 'categorical_crossentropy', metrics=['acc', 'Precision', 'Recall'])\n",
    "model.fit(x_train, \n",
    "          t_train_categorical, \n",
    "          epochs=n_epochs,\n",
    "          batch_size=batch_size,\n",
    "          verbose=0)\n",
    "\n",
    "# Lo evaluamos:\n",
    "_, acc, prec, recall = model.evaluate(x_test, t_test_categorical, verbose=0)\n",
    "f1_score = 2 * prec * recall / (prec + recall + 1e-8) # Añado un épsilon para evitar división entre 0\n",
    "print(\"TEST F1 SCORE:\", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avPX8NNDVJGS"
   },
   "source": [
    "## <font color=\"#CA3532\">Keras Tuner</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjeTdew4Qfpg"
   },
   "source": [
    "Keras Tuner es una librería que simplifica en gran medida el ajuste de los hiperparámetros de una red neuronal. \n",
    "\n",
    "**Keras tuner**: https://keras-team.github.io/keras-tuner/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOcg2zKmNYEN"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U keras-tuner\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3zuQAFqQtLd"
   },
   "source": [
    "Se define un hipermodelo, una función que genera un modelo en Keras que depende de unos hiperparámetros ``hp`` con los que vamos a jugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6krn1G8VHdI"
   },
   "outputs": [],
   "source": [
    "# hp.Choice elige entre los valores dados\n",
    "# hp.Int y hp.Float eligen entre un mínimo y un máximo\n",
    "\n",
    "def modelo(l2reg, num_units, activation, optimizer, seed=1):\n",
    "  model = keras.Sequential()\n",
    "  model.add(keras.layers.Input(shape=(30)))\n",
    "  model.add(keras.layers.Dense(units = num_units, activation=activation,\n",
    "                               kernel_regularizer=keras.regularizers.l2(l2reg),\n",
    "                               kernel_initializer=keras.initializers.GlorotNormal(seed=seed)))\n",
    "  model.add(keras.layers.Dense(2, activation=\"softmax\",\n",
    "                               kernel_initializer=keras.initializers.RandomNormal(0, 1, seed=seed)))\n",
    "\n",
    "  model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "  \n",
    "  return  model\n",
    "\n",
    "def hipermodelo(hp):\n",
    "  hp_l2reg = hp.Choice('l2reg', values = [1.0, 0.1, 0.01, 0.001, 0.0001]) \n",
    "  hp_num_units = hp.Choice('num_units', values = [5, 10, 15, 20]) \n",
    "  hp_act = hp.Choice('activation', values = ['sigmoid', 'relu']) \n",
    "  hp_opt = hp.Choice('optimizer', values = ['adam', 'sgd'])\n",
    "  \n",
    "  return modelo(hp_l2reg, hp_num_units, hp_act, hp_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blRRW6Hke6Xg"
   },
   "source": [
    "Hay diferentes algoritmos de búsqueda de hiperparámetros:\n",
    "\n",
    "https://keras.io/api/keras_tuner/tuners/\n",
    "\n",
    "Nosotros vamos a utilizar el algoritmo Hyperband: \n",
    "\n",
    "Li, Lisha, and Kevin Jamieson. \"Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization.\" Journal of Machine Learning Research 18 (2018): 1-52.\n",
    "\n",
    "https://keras.io/api/keras_tuner/tuners/hyperband/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isI8lddwNSMd"
   },
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(hipermodelo,\n",
    "                     objective = 'val_acc', # Métrica a optimizar\n",
    "                     max_epochs = 50, # Número de épocas máximo a entrenar cada modelo. O bien pones un valor alto con early stopping o bien pones un valor bajo\n",
    "                     factor = 3, # Cuantos modelos elimino de la búsqueda en cada bracket\n",
    "                     directory = 'my_dir',\n",
    "                     project_name = 'cancer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKGrCCvINbsp"
   },
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLne_xf7hZvd"
   },
   "outputs": [],
   "source": [
    "# Preparamos los datos con validación simple:\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "x = data.data\n",
    "t = data.target[:, None]\n",
    "\n",
    "x_train, x_test, t_train, t_test = train_test_split(x, t, test_size=0.2, random_state=42)\n",
    "x_train, x_val, t_train, t_val = train_test_split(x_train, t_train, test_size=0.2, random_state=30)\n",
    "\n",
    "t_train_categorical = to_categorical(t_train)\n",
    "t_val_categorical = to_categorical(t_val)\n",
    "t_test_categorical = to_categorical(t_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train_categorical.shape)\n",
    "print(x_val.shape)\n",
    "print(t_val_categorical.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test_categorical.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sz81qoUqNobL"
   },
   "outputs": [],
   "source": [
    "# En el EarlyStopping definimos la métrica a valorar, que coincide con el objetivo del tuner\n",
    "# Patience es el número de épocas que entrena sin mejora antes de parar\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping('val_acc', patience=5)]\n",
    "tuner.search(x_train, t_train_categorical,\n",
    "             validation_data=(x_val, t_val_categorical),\n",
    "             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3BFeOUFNyKr"
   },
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "best_hps.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df9zKVE5i9vR"
   },
   "source": [
    "Una vez tenemos los mejores hiperparámetros, los fijamos y entrenamos una única vez con todos los datos de training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAFvkcwFi8p6"
   },
   "outputs": [],
   "source": [
    "# Concatenamos datos de train y validacion\n",
    "final_x_train = np.concatenate((x_train, x_val), axis=0)\n",
    "final_t_train_categorical = np.concatenate((t_train_categorical, t_val_categorical), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGyvdp4njxU2"
   },
   "outputs": [],
   "source": [
    "l2reg = best_hps['l2reg']\n",
    "num_units = best_hps['num_units']\n",
    "activation = best_hps['activation']\n",
    "optimizer = best_hps['optimizer']\n",
    "epochs = 50\n",
    "model = modelo(l2reg, num_units, activation, optimizer)\n",
    "best_test_acc = 0.0\n",
    "for epoch in range(epochs):\n",
    "  history = model.fit(final_x_train, final_t_train_categorical, validation_data=(x_test, t_test_categorical))\n",
    "  if history.history['val_acc'][0] > best_test_acc:\n",
    "    best_test_acc = history.history['val_acc'][0]\n",
    "    model.save_weights('best')\n",
    "model.load_weights('best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMVdW7Uxk9RI"
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, t_test_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b62SGNn0lOql"
   },
   "source": [
    "Por último, vamos a darle un poco más de capacidad de búsqueda al algoritmo. Ya no queremos fijar los valores, sino que le dejamos que busque en un rango:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4NixupgmMOH"
   },
   "outputs": [],
   "source": [
    "def modelo_v2(l2reg, num_units, dropout, activation, learning_rate, num_capas_ocultas, seed=1):\n",
    "  model = keras.Sequential()\n",
    "  model.add(keras.layers.Input(shape=(30)))\n",
    "  for _ in range(num_capas_ocultas):\n",
    "    model.add(keras.layers.Dense(units = num_units, activation = activation, \n",
    "                                 kernel_regularizer=keras.regularizers.l2(l2reg),\n",
    "                                 kernel_initializer=keras.initializers.GlorotNormal(seed=seed)))\n",
    "    model.add(keras.layers.Dropout(rate = dropout))\n",
    "  model.add(keras.layers.Dense(2, activation=\"softmax\",\n",
    "                               kernel_initializer=keras.initializers.RandomNormal(0, 1, seed=seed)))\n",
    "\n",
    "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['acc', 'Precision', 'Recall'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WnqdIPy9OAxT"
   },
   "outputs": [],
   "source": [
    "def hipermodelo_v2(hp):\n",
    "  hp_l2reg = hp.Float('l2reg', min_value=1e-5, max_value=0.1, sampling='log') # Scale log permite hacer una búsqueda logaritmica\n",
    "  hp_num_units = hp.Int('num_units', min_value=5, max_value=50)\n",
    "  hp_dropout = hp.Float('dropout', min_value=0.0, max_value=0.5) # Dropout tiene una busqueda lineal\n",
    "  hp_act = hp.Choice('activation', values = ['sigmoid', 'relu']) \n",
    "  hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1.0, sampling=\"log\") # Igual que L2reg\n",
    "  hp_num_capas_ocultas = hp.Int('num_layers', min_value=1, max_value=5)\n",
    "  \n",
    "  return modelo_v2(hp_l2reg, hp_num_units, hp_dropout, hp_act, hp_learning_rate, hp_num_capas_ocultas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQbGlQJ6OnsO"
   },
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(hipermodelo_v2,\n",
    "                     objective = 'val_acc', \n",
    "                     max_epochs = 50,\n",
    "                     factor = 3,\n",
    "                     directory = 'my_dir',\n",
    "                     project_name = 'cancer_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnyttJ7uOsGm"
   },
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mHCOpdmOusE"
   },
   "outputs": [],
   "source": [
    "# En el EarlyStopping definimos la métrica a valorar, que coincide con el objetivo del tuner\n",
    "# Patience es el número de épocas que entrena sin mejora antes de parar\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping('val_acc', patience=5)]\n",
    "tuner.search(x_train, t_train_categorical,\n",
    "             validation_data=(x_val, t_val_categorical),\n",
    "             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PO_jXD2uPzL0"
   },
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "best_hps.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd8-IG1Jm41b"
   },
   "source": [
    "Una vez tenemos los mejores hiperparámetros, los fijamos y entrenamos una única vez con todos los datos de training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AEI-IPsm41d"
   },
   "outputs": [],
   "source": [
    "# Concatenamos datos de train y validacion\n",
    "final_x_train = np.concatenate((x_train, x_val), axis=0)\n",
    "final_t_train_categorical = np.concatenate((t_train_categorical, t_val_categorical), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkM90YcVm41d"
   },
   "outputs": [],
   "source": [
    "l2reg = best_hps['l2reg']\n",
    "num_units = best_hps['num_units']\n",
    "dropout = best_hps['dropout']\n",
    "activation = best_hps['activation']\n",
    "learning_rate = best_hps['learning_rate']\n",
    "num_capas_ocultas = best_hps['num_layers']\n",
    "model = modelo_v2(l2reg, num_units, dropout, activation, learning_rate, num_capas_ocultas)\n",
    "epochs = 50\n",
    "\n",
    "best_test_acc = 0.0\n",
    "for epoch in range(epochs):\n",
    "  history = model.fit(final_x_train, final_t_train_categorical, validation_data=(x_test, t_test_categorical))\n",
    "  if history.history['val_acc'][0] > best_test_acc:\n",
    "    best_test_acc = history.history['val_acc'][0]\n",
    "    model.save_weights('best')\n",
    "model.load_weights('best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5LLEv1bm41e"
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, t_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZ2FnmlHaC-w"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
